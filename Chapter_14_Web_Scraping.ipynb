{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14장 웹 스크레이핑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.1 웹 브라우저로 웹 사이트 접속하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w3school.com (html 태그에 대한 설명 참고)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하나의 웹 사이트에 접속하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 394페이지] naver 열기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser    # webbrowser 모듈 호출\n",
    "\n",
    "url = 'www.naver.com'   # 주소를 url을 변수에 직접 입력\n",
    "webbrowser.open_new(url)    # webbrowser.open() 함수로 url(인터넷주소)를 엶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 394~ 395페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser\n",
    "\n",
    "naver_search_url = \"http://search.naver.com/search.naver?query=\"  # 검색창\n",
    "search_word = '파이썬'                      # 파라미터(검색어)\n",
    "url = naver_search_url + search_word        # url = 검색착 + 파라미터\n",
    "\n",
    "webbrowser.open(url)      # 지정 url 열기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 검색어를 입력받아 사이트 열기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색할 단어를 입력하세요자연어처리\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser\n",
    "\n",
    "naver_search = \"http://search.naver.com/search.naver?query=\"\n",
    "keyword = input('검색할 단어를 입력하세요')\n",
    "\n",
    "webbrowser.open(naver_search + keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 395페이지] 구글 열기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser\n",
    "\n",
    "google_url = \"www.google.com/#q=\"   # 검색창\n",
    "search_word = '파이썬'              # 파라미터(검색어)\n",
    "url = google_url + search_word      # 검색창 url 주소\n",
    "\n",
    "webbrowser.open_new(url)            # url 열기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색어를 입력하세요: 자연어처리\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_url = \"www.google.com/#q=\"   # 검색창\n",
    "search_word = input('검색어를 입력하세요: ')            # 파라미터(검색어)\n",
    "url = google_url + search_word      # 검색창 url 주소\n",
    "\n",
    "webbrowser.open_new(url)            # url 열기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 여러 개의 웹 사이트에 접속하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 395페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "\n",
    "urls = ['www.naver.com', 'www.daum.net', 'www.google.com']  # url 주소 리스트\n",
    "\n",
    "for url in urls:               # urls 리스트의 반복변수 실행\n",
    "    webbrowser.open_new(url)   # url 주소 리스트의 주소창 모두 열기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 396페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "\n",
    "google_url = \"www.google.com/#q=\"\n",
    "search_words = ['natural language processing', 'computational linguistics']  # 검색어 리스트\n",
    "\n",
    "for search_word in search_words:  # 검색어 리스트의 반복변수 실행\n",
    "    webbrowser.open_new(google_url + search_word)   # 검색창 모두 열기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.2 웹 스크레이핑을 위한 기본 지식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터의 요청과 응답 과정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML의 기본 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 399페이지]**\n",
    "- 매직커멘드 %%writefile 로 html 문서 생성\n",
    "- html의 주석은 \\<!-- 주석 내용 --> 형식으로 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:\\myPyCode\\HTML_example.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile C:\\myPyCode\\HTML_example.html \n",
    "<!doctype html>\n",
    "<html>\n",
    " <head>\n",
    "  <meta charset=\"utf-8\">\n",
    "  <title>이것은 HTML 예제</title>\n",
    " </head>\n",
    " <body>\n",
    "  <h1>출간된 책 정보</h1>\n",
    "  <p id=\"book_title\">이해가 쏙쏙 되는 파이썬</p>  <!-- 같은 p값도 id 값을 지정해 구분할 수 있음 -->\n",
    "  <p id=\"author\">홍길동</p>  \n",
    "  <p id=\"publisher\">위키북스 출판사</p>\n",
    "  <p id=\"year\">2018</p>\n",
    " </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 저장된 html 열기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html>\n",
      "<html>\n",
      " <head>\n",
      "  <meta charset=\"utf-8\">\n",
      "  <title>�씠寃껋�� HTML �삁�젣</title>\n",
      " </head>\n",
      " <body>\n",
      "  <h1>異쒓컙�맂 梨� �젙蹂�</h1>\n",
      "  <p id=\"book_title\">�씠�빐媛� �룞�룞 �릺�뒗 �뙆�씠�뜫</p>  <!-- 媛숈�� p媛믩룄 id 媛믪쓣 吏��젙�빐 援щ텇�븷 �닔 �엳�쓬 -->\n",
      "  <p id=\"author\">�솉湲몃룞</p>  \n",
      "  <p id=\"publisher\">�쐞�궎遺곸뒪 異쒗뙋�궗</p>\n",
      "  <p id=\"year\">2018</p>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "!type C:\\myPyCode\\HTML_example.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html>\n",
      "<html>\n",
      " <head>\n",
      "  <meta charset=\"utf-8\">\n",
      "  <title>이것은 HTML 예제</title>\n",
      " </head>\n",
      " <body>\n",
      "  <h1>출간된 책 정보</h1>\n",
      "  <p id=\"book_title\">이해가 쏙쏙 되는 파이썬</p>  <!-- 같은 p값도 id 값을 지정해 구분할 수 있음 -->\n",
      "  <p id=\"author\">홍길동</p>  \n",
      "  <p id=\"publisher\">위키북스 출판사</p>\n",
      "  <p id=\"year\">2018</p>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('C:\\myPyCode\\HTML_example.html', encoding='utf-8') as f:\n",
    "    html = f.read()\n",
    "    \n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webbrowser.open_new('C:\\myPyCode\\HTML_example.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 400페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:/myPyCode/HTML_example2.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile C:/myPyCode/HTML_example2.html \n",
    "<!doctype html>\n",
    "<html>\n",
    " <head>\n",
    "  <meta charset=\"utf-8\">\n",
    "  <title>이것은 HTML 예제</title>\n",
    " </head>\n",
    " <body>\n",
    "  <h1>출간된 책 정보</h1>\n",
    "  <p>이해가 쏙쏙 되는 파이썬</p>   <!-- id값이 없을 경우 추출하기가 복잡해짐 -->\n",
    "  <p>홍길동</p>\n",
    "  <p>위키북스 출판사</p>  \n",
    "  <p>2018</p>\n",
    " </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 웹 페이지의 HTML 소스 갖고 오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 402페이지]**\n",
    "## requests()함수 -  웹페이지의 소스코드 복사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests     # 내장 모듈 실행\n",
    "\n",
    "r = requests.get(\"https://www.google.co.kr\")   # requests.get()함수로 해당주소의 소스코드를 가져옴\n",
    "r                                              # response 200, 실행됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 402페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!doctype html><html itemscope=\"\" itemtype=\"http://schema.org/WebPage\" lang=\"ko\"><head><meta content'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.text[0:100]   # 100개 글자만 출력해서 복사가 잘 되었는지 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 403페이지]**\n",
    "- a = requests.get(\"\")와 a.text를 한번에 실행\n",
    "### a = requests.get(\"\").text\n",
    "### a[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!doctype html><html itemscope=\"\" itemtype=\"http://schema.org/WebPage\" lang=\"ko\"><head><meta content'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "html = requests.get(\"https://www.google.co.kr\").text\n",
    "html[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML 소스코드를 분석하고 처리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 찾고 추출하기 \n",
    "- BeautifulSoup 모듈을 사용\n",
    "- bs4 모듈에서 BeautifulSoup 클래스 불러오기\n",
    "- 현재 사용하는 모듈은 뷰티풀수프4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 404페이지]**\n",
    "#### 뷰티풀수프 객체 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><body><div><span> <a href=\"http://www.naver.com\">naver</a> <a href=\"https://www.google.com\">google</a> <a href=\"http://www.daum.net/\">daum</a> </span></div></body></html>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 테스트용 html 코드 작성\n",
    "html = \"\"\"<html><body><div><span>\\\n",
    "        <a href=http://www.naver.com>naver</a>\\\n",
    "        <a href=https://www.google.com>google</a>\\\n",
    "        <a href=http://www.daum.net/>daum</a>\\\n",
    "        </span></div></body></html>\"\"\" \n",
    "\n",
    "# BeautifulSoup를 이용해 HTML 소스를 파싱\n",
    "soup = BeautifulSoup(html, 'lxml')   # BeautifulSoup 클래스로 soup 객체를 만듦\n",
    "                                     # BeautifulSoup(html텍스트, 파서이름)\n",
    "                                     # 생성된 html 텍스트와 'lxml' 파서를 파라미터로 입력\n",
    "soup                 # 해당 html코드가 뷰티풀소프 객체로 생성됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 404페이지]**\n",
    "# BeautifulSoup의 메소드 활용\n",
    "- 객체명.prettify() 들여쓰기를 포함에 문서출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <body>\n",
      "  <div>\n",
      "   <span>\n",
      "    <a href=\"http://www.naver.com\">\n",
      "     naver\n",
      "    </a>\n",
      "    <a href=\"https://www.google.com\">\n",
      "     google\n",
      "    </a>\n",
      "    <a href=\"http://www.daum.net/\">\n",
      "     daum\n",
      "    </a>\n",
      "   </span>\n",
      "  </div>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify())    # 뷰티풀소프 객체를 예쁘게 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 405페이지]**\n",
    "## .find() 메소드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"http://www.naver.com\">naver</a>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('a')   # 찾아야 할 태그 이름 입력\n",
    "                 # 첫번째 a 태그 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 405페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'naver'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('a').get_text()   # 시작, 종료 앵커태그 사이에 있는 텍스트 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 405페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"http://www.naver.com\">naver</a>,\n",
       " <a href=\"https://www.google.com\">google</a>,\n",
       " <a href=\"http://www.daum.net/\">daum</a>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('a')  # 객체 안에 있는 모든 a 태그를 리스트형태로 출력함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 406페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naver\n",
      "google\n",
      "daum\n"
     ]
    }
   ],
   "source": [
    "a_tags = soup.find_all('a')\n",
    "\n",
    "for site_name in a_tags:\n",
    "    print(site_name.get_text())  # a 태그의 텍스트만 선택하여 출력 (반복)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 406 ~ 407페이지]**\n",
    "## 태그 값 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup   # 클래스 호출\n",
    "\n",
    "# HTML 코드 파일 생성\n",
    "html2 = \"\"\"\n",
    "<html>\n",
    " <head>\n",
    "  <title>작품과 작가 모음</title>\n",
    " </head>\n",
    " <body>\n",
    "  <h1>책 정보</h1>\n",
    "  <p id=\"book_title\">토지</p>\n",
    "  <p id=\"author\">박경리</p>\n",
    "  \n",
    "  <p id=\"book_title\">태백산맥</p>\n",
    "  <p id=\"author\">조정래</p>\n",
    "\n",
    "  <p id=\"book_title\">감옥으로부터의 사색</p>\n",
    "  <p id=\"author\">신영복</p>\n",
    " </body>\n",
    "</html>\n",
    "\"\"\" \n",
    "\n",
    "soup2 = BeautifulSoup(html2, \"lxml\")   # html코드데이터를 뷰티풀소프 클래스를 통해 객체로 만듦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 407페이지]**\n",
    "## BeatifulSoup 클래스의 메소드 활용\n",
    "- 객체명.title 타이틀 태그 출력\n",
    "- 객체명.body 바디 태그 출력\n",
    "- 객체명.body.h 바디 태그 안의 h태그 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>작품과 작가 모음</title>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.title    # 객체의 title 태그 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 407페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<body>\n",
       "<h1>책 정보</h1>\n",
       "<p id=\"book_title\">토지</p>\n",
       "<p id=\"author\">박경리</p>\n",
       "<p id=\"book_title\">태백산맥</p>\n",
       "<p id=\"author\">조정래</p>\n",
       "<p id=\"book_title\">감옥으로부터의 사색</p>\n",
       "<p id=\"author\">신영복</p>\n",
       "</body>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.body  # 객체의 body 태그 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 407페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1>책 정보</h1>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.body.h1  # 객체의 body 태그의 h1 태그 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p id=\"book_title\">토지</p>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.body.p  # 객체의 body 태그의 첫번째 p태그 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 408페이지]**\n",
    "## .find() 메소드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"book_title\">토지</p>,\n",
       " <p id=\"author\">박경리</p>,\n",
       " <p id=\"book_title\">태백산맥</p>,\n",
       " <p id=\"author\">조정래</p>,\n",
       " <p id=\"book_title\">감옥으로부터의 사색</p>,\n",
       " <p id=\"author\">신영복</p>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.find_all('p')  # 객체에서 p태그 모두 찾기 (리스트)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 408페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p id=\"book_title\">토지</p>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.find('p', {\"id\":\"book_title\"})   # 객체에서 p태그 이면서, 속성이름이 id이면서, 속성값이 book_title인 첫번째 태그 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 408페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p id=\"author\">박경리</p>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.find('p', {\"id\":\"author\"})  # 객체에서 p태그이면서 속성이름이 id, 속성값이 author인 첫번째 태그 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 408 ~ 409페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"book_title\">토지</p>,\n",
       " <p id=\"book_title\">태백산맥</p>,\n",
       " <p id=\"book_title\">감옥으로부터의 사색</p>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.find_all('p', {\"id\":\"book_title\"})  # 객체 안에서 p태그이면서, 속성값 id, 속성이름 book_title인 모든 태그 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"author\">박경리</p>, <p id=\"author\">조정래</p>, <p id=\"author\">신영복</p>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.find_all('p', {\"id\":\"author\"})  # 객체 안에서 p태그이면서, 속성값 id, 속성이름 author인 모든 태그 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 409페이지]**\n",
    "## 선택한 태그에서 텍스트만 불러오기\n",
    " - for 반복문 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토지/박경리\n",
      "태백산맥/조정래\n",
      "감옥으로부터의 사색/신영복\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup   # 클래스 호출\n",
    "\n",
    "soup2 = BeautifulSoup(html2, \"lxml\")   # 파싱\n",
    "\n",
    "book_titles = soup2.find_all('p', {\"id\":\"book_title\"})  # p태그 중 속성명이 id이면서 속성값이 book_title인 모든 태그 불러오기 \n",
    "authors = soup2.find_all('p', {\"id\":\"author\"})          # p태그 중 속성명이 id이면서 속성값이 author인 모든 태그 불러오기 \n",
    "\n",
    "for book_title, author in zip(book_titles, authors):         # for반복문으로 두 태그의 텍스트만 선택해서 출력 (반복)\n",
    "    print(book_title.get_text() + '/' + author.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 410페이지]**\n",
    "## .select() 메소드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h1>책 정보</h1>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.select('body h1')  # body 태그 안의 h1 태그 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 410페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"book_title\">토지</p>,\n",
       " <p id=\"author\">박경리</p>,\n",
       " <p id=\"book_title\">태백산맥</p>,\n",
       " <p id=\"author\">조정래</p>,\n",
       " <p id=\"book_title\">감옥으로부터의 사색</p>,\n",
       " <p id=\"author\">신영복</p>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.select('body p')  # body 태그 안의 p태그 출력 (모든 값 출력)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p id=\"book_title\">토지</p>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.body.p    # 이렇게 지정하면 첫번째 값만 출력함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"book_title\">토지</p>,\n",
       " <p id=\"author\">박경리</p>,\n",
       " <p id=\"book_title\">태백산맥</p>,\n",
       " <p id=\"author\">조정래</p>,\n",
       " <p id=\"book_title\">감옥으로부터의 사색</p>,\n",
       " <p id=\"author\">신영복</p>]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.find_all('p')   # 모든 p태그는 불러올수 있지만 내포관계를 설정해줄 수 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 410페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"book_title\">토지</p>,\n",
       " <p id=\"author\">박경리</p>,\n",
       " <p id=\"book_title\">태백산맥</p>,\n",
       " <p id=\"author\">조정래</p>,\n",
       " <p id=\"book_title\">감옥으로부터의 사색</p>,\n",
       " <p id=\"author\">신영복</p>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.select('p')    # 모든 p태그 출력, .find_all('p') 메소드와 동일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 410 ~ 411페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"book_title\">토지</p>,\n",
       " <p id=\"book_title\">태백산맥</p>,\n",
       " <p id=\"book_title\">감옥으로부터의 사색</p>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.select('body p#book_title')  # body태그 안의 p 태그의 id값이 booktitle인 p 태그만 출력 (# id, . class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"book_title\">토지</p>,\n",
       " <p id=\"book_title\">태백산맥</p>,\n",
       " <p id=\"book_title\">감옥으로부터의 사색</p>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.find_all('p', {'id':'book_title'})    # 동일한 방법 (p태그의 내포관계 설정 불가)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"author\">박경리</p>, <p id=\"author\">조정래</p>, <p id=\"author\">신영복</p>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.select('p#author')  # p태그 안에 id값이 author인 태그 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"author\">박경리</p>, <p id=\"author\">조정래</p>, <p id=\"author\">신영복</p>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.select('body p#author')  # body 태그 안의 p태그 안에 id값이 author인 태그 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"author\">박경리</p>, <p id=\"author\">조정래</p>, <p id=\"author\">신영복</p>]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.find_all('p', {'id':'author'})   # 동일한 방법, 하지만 p태그의 내포관계 설정 불가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 411페이지]**\n",
    "## 연습예제\n",
    "#### html 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:/myPyCode/HTML_example_my_site.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile C:/myPyCode/HTML_example_my_site.html \n",
    "<!doctype html>\n",
    "<html>\n",
    "  <head>\n",
    "    <meta charset=\"utf-8\">\n",
    "    <title>사이트 모음</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <p id=\"title\"><b>자주 가는 사이트 모음</b></p>\n",
    "    <p id=\"contents\">이곳은 자주 가는 사이트를 모아둔 곳입니다.</p>\n",
    "    <a href=\"http://www.naver.com\" class=\"portal\" id=\"naver\">네이버</a> <br>\n",
    "    <a href=\"https://www.google.com\" class=\"search\" id=\"google\">구글</a> <br>\n",
    "    <a href=\"http://www.daum.net\" class=\"portal\" id=\"danum\">다음</a> <br>\n",
    "    <a href=\"http://www.nl.go.kr\" class=\"government\" id=\"nl\">국립중앙도서관</a>\n",
    "  </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 412페이지]**\n",
    "#### html 파일 불러오기 (9장 참고)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<!DOCTYPE html>\n",
       "<html>\n",
       "<head>\n",
       "<meta charset=\"utf-8\"/>\n",
       "<title>사이트 모음</title>\n",
       "</head>\n",
       "<body>\n",
       "<p id=\"title\"><b>자주 가는 사이트 모음</b></p>\n",
       "<p id=\"contents\">이곳은 자주 가는 사이트를 모아둔 곳입니다.</p>\n",
       "<a class=\"portal\" href=\"http://www.naver.com\" id=\"naver\">네이버</a> <br/>\n",
       "<a class=\"search\" href=\"https://www.google.com\" id=\"google\">구글</a> <br/>\n",
       "<a class=\"portal\" href=\"http://www.daum.net\" id=\"danum\">다음</a> <br/>\n",
       "<a class=\"government\" href=\"http://www.nl.go.kr\" id=\"nl\">국립중앙도서관</a>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('C:/myPyCode/HTML_example_my_site.html', encoding='utf-8')  # 한글파일은 인코딩 필수\n",
    "\n",
    "html3 = f.read()   # 모든 텍스트를 읽어서 문자열 변수로 저장\n",
    "f.close()\n",
    "\n",
    "soup3 = BeautifulSoup(html3, \"lxml\")   # html 문자열로 BeautifulSoup 객체를 생성\n",
    "soup3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 412페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"portal\" href=\"http://www.naver.com\" id=\"naver\">네이버</a>,\n",
       " <a class=\"search\" href=\"https://www.google.com\" id=\"google\">구글</a>,\n",
       " <a class=\"portal\" href=\"http://www.daum.net\" id=\"danum\">다음</a>,\n",
       " <a class=\"government\" href=\"http://www.nl.go.kr\" id=\"nl\">국립중앙도서관</a>]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup3.select('a')   # 객체에서 모든 앵커태그 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 412페이지]**\n",
    "- . 클래스\n",
    "- \\# 아이디"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"portal\" href=\"http://www.naver.com\" id=\"naver\">네이버</a>,\n",
       " <a class=\"portal\" href=\"http://www.daum.net\" id=\"danum\">다음</a>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup3.select('a.portal')   # 모든 앵커태그중 class 값이 portal인 태그 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup3.select('a#portal')  # 모든앵커태그중 id 값이  portal인 태그 추출 (없음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"portal\" href=\"http://www.naver.com\" id=\"naver\">네이버</a>]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup3.select('a#naver')   # 모든 앵커태그중 id값이 naver이 태그 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 웹 브라우저의 요소 검사"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 414페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"portal\" href=\"http://www.naver.com\" id=\"naver\">네이버</a>,\n",
       " <a class=\"search\" href=\"https://www.google.com\" id=\"google\">구글</a>,\n",
       " <a class=\"portal\" href=\"http://www.daum.net\" id=\"danum\">다음</a>,\n",
       " <a class=\"government\" href=\"http://www.nl.go.kr\" id=\"nl\">국립중앙도서관</a>]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup3.select('a')   # 모든 a 태그 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 414페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"portal\" href=\"http://www.naver.com\" id=\"naver\">네이버</a>,\n",
       " <a class=\"portal\" href=\"http://www.daum.net\" id=\"danum\">다음</a>]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup3.select('a.portal')  # 모든 a 태그 중 class 값이 portal인 태그 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 415페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"portal\" href=\"http://www.naver.com\" id=\"naver\">네이버</a>]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup3.select('a#naver')  # 모든 a 태그 중 id값이 naver인 태그 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 줄 바꿈으로 가독성 높이기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 415페이지]**\n",
    "#### html 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:/myPyCode/br_example_constitution.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile C:/myPyCode/br_example_constitution.html\n",
    "<!doctype html>\n",
    "<html>\n",
    "  <head>\n",
    "    <meta charset=\"utf-8\">\n",
    "    <title>줄 바꿈 테스트 예제</title>\n",
    "  </head>\n",
    "  <body>\n",
    "  <p id=\"title\"><b>대한민국헌법</b></p>\n",
    "  <p id=\"content\">제1조 <br/>①대한민국은 민주공화국이다.<br/>②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.</p>\n",
    "  <p id=\"content\">제2조 <br/>①대한민국의 국민이 되는 요건은 법률로 정한다.<br/>②국가는 법률이 정하는 바에 의하여 재외국민을 보호할 의무를 진다.</p>\n",
    "  </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 416페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대한민국헌법\n",
      "제1조 ①대한민국은 민주공화국이다.②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.\n",
      "제2조 ①대한민국의 국민이 되는 요건은 법률로 정한다.②국가는 법률이 정하는 바에 의하여 재외국민을 보호할 의무를 진다.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "f = open('C:/myPyCode/br_example_constitution.html', encoding='utf-8')  # 파일 열기\n",
    "html_1 = f.read()    # 파일 읽기\n",
    "f.close()     # 파일 닫기\n",
    "\n",
    "soup = BeautifulSoup(html_1, \"lxml\")   # 읽어온 html파일을 뷰티풀소프 객체로 생성\n",
    "\n",
    "title = soup.find('p', {\"id\":\"title\"})            # p태그중 id값이 title인 첫번째 태그를 리스트로 생성\n",
    "contents = soup.find_all('p', {\"id\":\"content\"})   # p태그중 id값이 content인 모든 태그를 리스트로 생성\n",
    "\n",
    "print(title.get_text())   # 첫번째로 추출한 태그 출력\n",
    "for content in contents:\n",
    "    print(content.get_text())  # 다음으로 추출한 모든 태그 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 417페이지]**\n",
    "# 문자열 치환\n",
    "- 객체명.replace_with(\"\\n\") 객체의 태그를 뉴라인문자로 치환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <body>\n",
      "  <p id=\"content\">\n",
      "   제1조\n",
      "   <br/>\n",
      "   ①대한민국은 민주공화국이다.\n",
      "   <br/>\n",
      "   ②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "html1 = '<p id=\"content\">제1조 <br/>①대한민국은 민주공화국이다.<br/>②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.</p>'\n",
    "\n",
    "soup1 = BeautifulSoup(html1, 'lxml')\n",
    "print(soup1.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 연습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 태그 p로 찾은 요소\n",
      "<p id=\"content\">제1조 <br/>①대한민국은 민주공화국이다.<br/>②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.</p>\n",
      "==> 결과에서 태그 br로 찾은 요소: <br/>\n",
      "==> 태그 br을 개행문자로 바꾼 결과\n",
      "<p id=\"content\">제1조 \n",
      "①대한민국은 민주공화국이다.<br/>②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.</p>\n"
     ]
    }
   ],
   "source": [
    "html1 = '<p id=\"content\">제1조 <br/>①대한민국은 민주공화국이다.<br/>②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.</p>'\n",
    "\n",
    "soup1 = BeautifulSoup(html1, \"lxml\")   # html 문자열을 뷰티풀소프 객체로 생성\n",
    "\n",
    "print(\"==> 태그 p로 찾은 요소\")\n",
    "content1 = soup1.find('p', {\"id\":\"content\"})   # p태그 중 id가 content인 첫번째 태그 선택\n",
    "print(content1)                                # 출력\n",
    "\n",
    "br_content = content1.find(\"br\")     # content1에서 br태그 찾기 (태그만 선택)\n",
    "print(\"==> 결과에서 태그 br로 찾은 요소:\", br_content)  # 출력\n",
    "\n",
    "br_content.replace_with(\"\\n\")    # <br/>을 \\n으로 변경\n",
    "print(\"==> 태그 br을 개행문자로 바꾼 결과\")\n",
    "print(content1)   # 치환한 객체 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 418페이지]**\n",
    "- 실전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p id=\"content\">제1조 \n",
      "①대한민국은 민주공화국이다.\n",
      "②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.</p>\n"
     ]
    }
   ],
   "source": [
    "soup2 = BeautifulSoup(html1, \"lxml\")    # html 문자열을 뷰티풀소프 객체로 생성\n",
    "content2 = soup2.find('p', {\"id\":\"content\"})   # p태그 중에서 id가 content인 첫번째 태그 선택\n",
    "\n",
    "br_contents = content2.find_all(\"br\")    #content2 객체에서 br태그를 모두 찾아서 리스트로 생성\n",
    "\n",
    "for br_content in br_contents:           # for 반복문\n",
    "    br_content.replace_with(\"\\n\")        # 객체의 태그를 뉴라인문자로 변경\n",
    "print(content2)                          # 출력 (<br>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 418페이지]**\n",
    "- 함수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_newline(soup_html):\n",
    "    br_to_newlines = soup_html.find_all(\"br\")\n",
    "    for br_to_newline in br_to_newlines: \n",
    "        br_to_newline.replace_with(\"\\n\")\n",
    "    return soup_html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 418페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><body><p id=\"content\">제1조 \n",
       "①대한민국은 민주공화국이다.\n",
       "②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.</p></body></html>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2 = BeautifulSoup(html1, \"lxml\")\n",
    "content2 = soup2.find('p', {\"id\":\"content\"})\n",
    "content3 = replace_newline(content2)\n",
    "#print(content3.get_text())\n",
    "soup2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 419페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제1조 \n",
      "①대한민국은 민주공화국이다.\n",
      "②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html1, \"lxml\")\n",
    "\n",
    "title = soup.find('p', {\"id\":\"title\"})\n",
    "contents = soup.find_all('p', {\"id\":\"content\"})\n",
    "\n",
    "#print(title.get_text(), '\\n')\n",
    "\n",
    "for content in contents:\n",
    "    content1 = replace_newline(content)\n",
    "    print(content1.get_text(),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.3 웹 사이트에서 데이터 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 웹 스크레이핑 시 주의 사항"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 순위 데이터를 가져오기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 웹 사이트 순위"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 423페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests     # 소스코드 복사 모듈 호출\n",
    "from bs4 import BeautifulSoup    # 뷰티풀소프 객체 생성 클래스(BeautifulSoup) 호출\n",
    "\n",
    "url = \"https://www.alexa.com/topsites/countries/KR\"   # 소스코드 복사할 사이트\n",
    "\n",
    "html_website_ranking = requests.get(url).text     # 해당 페이지의 소스코드 저장 (문자열)\n",
    "soup_website_ranking = BeautifulSoup(html_website_ranking, \"lxml\")   # 소스코드를 뷰티풀소프 객체로 생성\n",
    "\n",
    "website_ranking = soup_website_ranking.select('p a') # p 태그 안의 a 태그를 찾아 리스트에 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 423페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"https://support.alexa.com/hc/en-us/articles/200444340\" target=\"_blank\">this explanation</a>,\n",
       " <a href=\"/siteinfo/google.com\">Google.com</a>,\n",
       " <a href=\"/siteinfo/naver.com\">Naver.com</a>,\n",
       " <a href=\"/siteinfo/youtube.com\">Youtube.com</a>,\n",
       " <a href=\"/siteinfo/daum.net\">Daum.net</a>,\n",
       " <a href=\"/siteinfo/tmall.com\">Tmall.com</a>,\n",
       " <a href=\"/siteinfo/tistory.com\">Tistory.com</a>,\n",
       " <a href=\"/siteinfo/google.co.kr\">Google.co.kr</a>,\n",
       " <a href=\"/siteinfo/sohu.com\">Sohu.com</a>,\n",
       " <a href=\"/siteinfo/qq.com\">Qq.com</a>,\n",
       " <a href=\"/siteinfo/facebook.com\">Facebook.com</a>,\n",
       " <a href=\"/siteinfo/login.tmall.com\">Login.tmall.com</a>,\n",
       " <a href=\"/siteinfo/taobao.com\">Taobao.com</a>,\n",
       " <a href=\"/siteinfo/wikipedia.org\">Wikipedia.org</a>,\n",
       " <a href=\"/siteinfo/namu.wiki\">Namu.wiki</a>,\n",
       " <a href=\"/siteinfo/jd.com\">Jd.com</a>,\n",
       " <a href=\"/siteinfo/amazon.com\">Amazon.com</a>,\n",
       " <a href=\"/siteinfo/kakao.com\">Kakao.com</a>,\n",
       " <a href=\"/siteinfo/360.cn\">360.cn</a>,\n",
       " <a href=\"/siteinfo/dcinside.com\">Dcinside.com</a>,\n",
       " <a href=\"/siteinfo/baidu.com\">Baidu.com</a>,\n",
       " <a href=\"/siteinfo/netflix.com\">Netflix.com</a>,\n",
       " <a href=\"/siteinfo/coupang.com\">Coupang.com</a>,\n",
       " <a href=\"/siteinfo/pages.tmall.com\">Pages.tmall.com</a>,\n",
       " <a href=\"/siteinfo/blog.me\">Blog.me</a>,\n",
       " <a href=\"/siteinfo/weibo.com\">Weibo.com</a>,\n",
       " <a href=\"/siteinfo/sina.com.cn\">Sina.com.cn</a>,\n",
       " <a href=\"/siteinfo/11st.co.kr\">11st.co.kr</a>,\n",
       " <a href=\"/siteinfo/torrentwal.com\">Torrentwal.com</a>,\n",
       " <a href=\"/siteinfo/gmarket.co.kr\">Gmarket.co.kr</a>,\n",
       " <a href=\"/siteinfo/apple.com\">Apple.com</a>,\n",
       " <a href=\"/siteinfo/stackoverflow.com\">Stackoverflow.com</a>,\n",
       " <a href=\"/siteinfo/donga.com\">Donga.com</a>,\n",
       " <a href=\"/siteinfo/microsoft.com\">Microsoft.com</a>,\n",
       " <a href=\"/siteinfo/yahoo.com\">Yahoo.com</a>,\n",
       " <a href=\"/siteinfo/twitch.tv\">Twitch.tv</a>,\n",
       " <a href=\"/siteinfo/ebay.com\">Ebay.com</a>,\n",
       " <a href=\"/siteinfo/adobe.com\">Adobe.com</a>,\n",
       " <a href=\"/siteinfo/inven.co.kr\">Inven.co.kr</a>,\n",
       " <a href=\"/siteinfo/auction.co.kr\">Auction.co.kr</a>,\n",
       " <a href=\"/siteinfo/office.com\">Office.com</a>,\n",
       " <a href=\"/siteinfo/clien.net\">Clien.net</a>,\n",
       " <a href=\"/siteinfo/nexon.com\">Nexon.com</a>,\n",
       " <a href=\"/siteinfo/amazon.co.jp\">Amazon.co.jp</a>,\n",
       " <a href=\"/siteinfo/bing.com\">Bing.com</a>,\n",
       " <a href=\"/siteinfo/dropbox.com\">Dropbox.com</a>,\n",
       " <a href=\"/siteinfo/tumblr.com\">Tumblr.com</a>,\n",
       " <a href=\"/siteinfo/nate.com\">Nate.com</a>,\n",
       " <a href=\"/siteinfo/instagram.com\">Instagram.com</a>,\n",
       " <a href=\"/siteinfo/interpark.com\">Interpark.com</a>,\n",
       " <a href=\"/siteinfo/amazon.co.uk\">Amazon.co.uk</a>]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_ranking  # 저장된 리스트 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"https://support.alexa.com/hc/en-us/articles/200444340\" target=\"_blank\">this explanation</a>,\n",
       " <a href=\"/siteinfo/google.com\">Google.com</a>,\n",
       " <a href=\"/siteinfo/naver.com\">Naver.com</a>,\n",
       " <a href=\"/siteinfo/youtube.com\">Youtube.com</a>,\n",
       " <a href=\"/siteinfo/daum.net\">Daum.net</a>]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_ranking[0:5]   # 첫번째 값부터 다섯번째 값까지 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 424페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this explanation'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_ranking[0].get_text()  # 리스트 첫번째 값의 텍스트만 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 424페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# website_ranking_address = [website_ranking_element.get_text() for website_ranking_element in website_ranking]  # for루프로 반복실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this explanation',\n",
       " 'Google.com',\n",
       " 'Naver.com',\n",
       " 'Youtube.com',\n",
       " 'Daum.net',\n",
       " 'Tmall.com',\n",
       " 'Tistory.com',\n",
       " 'Google.co.kr',\n",
       " 'Sohu.com',\n",
       " 'Qq.com',\n",
       " 'Facebook.com',\n",
       " 'Login.tmall.com',\n",
       " 'Taobao.com',\n",
       " 'Wikipedia.org',\n",
       " 'Namu.wiki',\n",
       " 'Jd.com',\n",
       " 'Amazon.com',\n",
       " 'Kakao.com',\n",
       " '360.cn',\n",
       " 'Dcinside.com',\n",
       " 'Baidu.com',\n",
       " 'Netflix.com',\n",
       " 'Coupang.com',\n",
       " 'Pages.tmall.com',\n",
       " 'Blog.me',\n",
       " 'Weibo.com',\n",
       " 'Sina.com.cn',\n",
       " '11st.co.kr',\n",
       " 'Torrentwal.com',\n",
       " 'Gmarket.co.kr',\n",
       " 'Apple.com',\n",
       " 'Stackoverflow.com',\n",
       " 'Donga.com',\n",
       " 'Microsoft.com',\n",
       " 'Yahoo.com',\n",
       " 'Twitch.tv',\n",
       " 'Ebay.com',\n",
       " 'Adobe.com',\n",
       " 'Inven.co.kr',\n",
       " 'Auction.co.kr',\n",
       " 'Office.com',\n",
       " 'Clien.net',\n",
       " 'Nexon.com',\n",
       " 'Amazon.co.jp',\n",
       " 'Bing.com',\n",
       " 'Dropbox.com',\n",
       " 'Tumblr.com',\n",
       " 'Nate.com',\n",
       " 'Instagram.com',\n",
       " 'Interpark.com',\n",
       " 'Amazon.co.uk']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_ranking_values = []   # 빈 리스트 생성\n",
    "\n",
    "for website_ranking_value in website_ranking:     # for 반복문\n",
    "    website_ranking_values.append(website_ranking_value.get_text())\n",
    "    \n",
    "website_ranking_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 424페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this explanation', 'Google.com', 'Naver.com', 'Youtube.com', 'Daum.net']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_ranking_values[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Google.com', 'Naver.com', 'Youtube.com', 'Daum.net', 'Tmall.com']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del website_ranking_values[0]   # 무의미한 리스트의 첫번째 값 삭제\n",
    "\n",
    "website_ranking_values[0:5]   # 재출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[한국에서 방문객이 가장 높은 웹사이트 top 5]\n",
      "1: Google.com\n",
      "2: Naver.com\n",
      "3: Youtube.com\n",
      "4: Daum.net\n",
      "5: Tmall.com\n"
     ]
    }
   ],
   "source": [
    "print(\"[한국에서 방문객이 가장 높은 웹사이트 top 5]\")\n",
    "for a in range(5):\n",
    "    print(\"{0}: {1}\".format(a+1, website_ranking_values[a]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 424페이지]**\n",
    "## 한번에 실행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[한국에서 방문객 수 가장 많은 웹사이트 TOP5]\n",
      "1: Google.com\n",
      "2: Naver.com\n",
      "3: Youtube.com\n",
      "4: Daum.net\n",
      "5: Tmall.com\n"
     ]
    }
   ],
   "source": [
    "import requests                 # 소스코드 복사 모듈 호출\n",
    "from bs4 import BeautifulSoup   # html 파싱 모듈 호출\n",
    "\n",
    "url = \"https://www.alexa.com/topsites/countries/KR\"    # 소스코드를 복사할 주소\n",
    "\n",
    "html_website_ranking = requests.get(url).text    # 해당 주소의 소스코드를 텍스트문서로 변수에 저장\n",
    "soup_website_ranking = BeautifulSoup(html_website_ranking, \"lxml\")    # html텍스트문서를 뷰티풀소프 객체로 생성\n",
    "\n",
    "\n",
    "website_ranking = soup_website_ranking.select('p a')   # p 태그안의 a 태그 요소 선택(랭킹 사이트명)\n",
    "\n",
    "\n",
    "website_ranking_values = []   # 빈 리스트 생성\n",
    "for website_ranking_value in website_ranking:     # for 반복문\n",
    "    website_ranking_values.append(website_ranking_value.get_text())\n",
    "    \n",
    "    \n",
    "#무의미한 첫번째 리스트값 삭제\n",
    "del website_ranking_values[0]\n",
    "\n",
    "print(\"[한국에서 방문객 수 가장 많은 웹사이트 TOP5]\")\n",
    "for a in range(5):\n",
    "    print(\"{0}: {1}\".format(a+1, website_ranking_values[a]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 425페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Google.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naver.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Youtube.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Daum.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tmall.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tistory.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Website\n",
       "1   Google.com\n",
       "2    Naver.com\n",
       "3  Youtube.com\n",
       "4     Daum.net\n",
       "5    Tmall.com\n",
       "6  Tistory.com"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "website_ranking_dict = {'Website': website_ranking_values}   # 데이터프레임에 넣기 위해 딕셔너리로생성\n",
    "df = pd.DataFrame(website_ranking_dict, columns=['Website'], index=range(1,len(website_ranking_values)+1))\n",
    "df[0:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주간 음악 순위"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 429페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"ellipsis\">눈 (Feat. 이문세)</span>,\n",
       " <span class=\"ellipsis\">기억의 빈자리</span>,\n",
       " <span class=\"ellipsis\">선물</span>,\n",
       " <span class=\"ellipsis\">Beautiful</span>,\n",
       " <span class=\"ellipsis\">좋아</span>,\n",
       " <span class=\"ellipsis\">피카부 (Peek-A-Boo)</span>,\n",
       " <span class=\"ellipsis\">좋니</span>]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://music.naver.com/listen/history/index.nhn?type=TOTAL&year=2017&month=12&week=1\"\n",
    "html_music = requests.get(url).text\n",
    "soup_music = BeautifulSoup(html_music, \"lxml\")\n",
    "\n",
    "# a 태그의 요소 중에서 class 속성값이 \"_title\" 인 것을 찾고\n",
    "# 그 안에서 span 태그의 요소 중에서 class 속성값이 \"ellipsis\"인 요소를 추출\n",
    "titles = soup_music.select('a._title span.ellipsis') \n",
    "titles[0:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 429페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_titles = [title.get_text() for title in titles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 429페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['눈 (Feat. 이문세)', '기억의 빈자리', '선물', 'Beautiful', '좋아', '피카부 (Peek-A-Boo)', '좋니']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_titles[0:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 431페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\r\\n\\t\\t\\tZion.T\\r\\n\\t\\t'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a 태그의 요소 중에서 class 속성값이 \"_artist\" 인 것을 찾고\n",
    "# 그 안에서 span 태그의 요소 중에서 class 속성값이 \"ellipsis\"인 요소를 추출\n",
    "artists = soup_music.select('a._artist span.ellipsis') \n",
    "artists[0].get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 431페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Zion.T'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artists[0].get_text().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 431페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_artists = [artist.get_text().strip() for artist in artists]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 431페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Zion.T',\n",
       " '나얼',\n",
       " '멜로망스(Melomance)',\n",
       " 'Wanna One(워너원)',\n",
       " 'Red Velvet (레드벨벳)',\n",
       " '윤종신',\n",
       " '뉴이스트 W']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_artists[0:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 433페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# td 태그의 요소 중에서 class 속성값이 \"_artist\" 인 것을 찾고\n",
    "# 그 안에서 a 태그의 요소를 추출\n",
    "artists = soup_music.select('td._artist a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 433페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"_artist NPI=a:artist,r:1,i:115967\" href=\"/artist/home.nhn?artistId=115967\" title=\"Zion.T\">\n",
       "<span class=\"ellipsis\">\n",
       "\t\t\t\n",
       "\t\t\t\n",
       "\t\t\tZion.T\n",
       "\t\t</span>\n",
       "</a>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a alt=\"\" class=\"NPI=a:layerbtn,r:5\" href=\"javascript:void(0);\" title=\"\">민서</a>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artists[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 433페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Zion.T'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artists[0].get_text().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'민서'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artists[4].get_text().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 433페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_artists = [artist.get_text().strip() for artist in artists]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 434페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Zion.T',\n",
       " '나얼',\n",
       " '멜로망스(Melomance)',\n",
       " 'Wanna One(워너원)',\n",
       " '민서',\n",
       " 'Red Velvet (레드벨벳)',\n",
       " '윤종신']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_artists[0:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 434 ~ 435페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 눈 (Feat. 이문세) / Zion.T\n",
      "2: 기억의 빈자리 / 나얼\n",
      "3: 선물 / 멜로망스(Melomance)\n",
      "4: Beautiful / Wanna One(워너원)\n",
      "5: 좋아 / 민서\n",
      "6: 피카부 (Peek-A-Boo) / Red Velvet (레드벨벳)\n",
      "7: 좋니 / 윤종신\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://music.naver.com/listen/history/index.nhn?type=DOMESTIC&year=2017&month=12&week=1&page=1\"    \n",
    "# url = \"http://music.naver.com/listen/history/index.nhn?type=DOMESTIC&year=2017&month=12&week=1&page=2\"\n",
    "# url = \"http://music.naver.com/listen/top100.nhn?domain=TOTAL&page=1\"\n",
    "    \n",
    "html_music = requests.get(url).text\n",
    "soup_music = BeautifulSoup(html_music, \"lxml\")\n",
    "\n",
    "titles = soup_music.select('a._title span.ellipsis') \n",
    "artists = soup_music.select('td._artist a')\n",
    "\n",
    "music_titles = [title.get_text() for title in titles]\n",
    "music_artists = [artist.get_text().strip() for artist in artists]\n",
    "\n",
    "for k in range(7):\n",
    "    print(\"{0}: {1} / {2}\".format(k+1, music_titles[k], music_artists[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 435페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_titles_artists={}\n",
    "order = 0\n",
    "\n",
    "for (music_title, music_artist) in zip(music_titles, music_artists):\n",
    "    order = order + 1\n",
    "    music_titles_artists[order] = [music_title, music_artist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 436페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['눈 (Feat. 이문세)', 'Zion.T']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_titles_artists[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['기억의 빈자리', '나얼']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_titles_artists[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 436 ~ 437페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import glob\n",
    "    \n",
    "naver_music_url = \"http://music.naver.com/listen/history/index.nhn?type=DOMESTIC&year=2017&month=12&week=1&page=\"\n",
    " \n",
    "# 주소를 입력값으로 받아 top100 노래제목명, 가수명을 반환하는 함수 생성\n",
    "def naver_music(url):    \n",
    "    html_music = requests.get(url).text              # 소스코드 복사\n",
    "    soup_music = BeautifulSoup(html_music, \"lxml\")   # 소스코드 파싱\n",
    "\n",
    "    titles = soup_music.select('a._title span.ellipsis')   # 타이틀태그를 리스트에 저장\n",
    "    artists = soup_music.select('td._artist a')            # 아티스트태그를 리스트에 저장\n",
    "\n",
    "    music_titles = [title.get_text() for title in titles]    # (타이틀태그리스트의 각각의 원소에서 문자만 남김)을 반복,새 리스트생성\n",
    "    music_artists = [artist.get_text().strip() for artist in artists]\n",
    "                                                           # (아티스트태그리스트의 각각의 원소에서 문자만 남김)을 반복,새 리스트생성\n",
    "    \n",
    "    return music_titles, music_artists                 # 함수의 리턴값은 타이틀문자리스트, 아티스트문자리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/myPyCode/data/NaverMusicTop100.txt']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'C:/myPyCode/data/NaverMusicTop100.txt'   # 파일을 저장할 위치\n",
    "\n",
    "f = open(file_name,'w') # 파일 열기, 엶과 동시에 파일 생성\n",
    "\n",
    "# 각 page에서 50개의 노래 제목과 아티스트 추출\n",
    "for page in range(2):      # 2번 반복하는 for반복문\n",
    "    naver_music_url_page = naver_music_url + str(page+1) # 첫번째(0+1= 1, 1페이지 출력), 두번째(1+1=2, 2페이지 출력)\n",
    "    naver_music_titles, naver_music_artists = naver_music(naver_music_url_page) # 함수의 반환값을 각각의 이름으로 지정\n",
    "    \n",
    "    # 추출된 노래 제목과 아티스트를 파일에 저장 \n",
    "    for k in range(len(naver_music_titles)):\n",
    "        f.write(\"{0:2d}: {1}/{2}\\n\".format(page*50 + k+1, naver_music_titles[k],  naver_music_artists[k])) #1~50,51~100번호 생성\n",
    "        \n",
    "f.close() # 파일 작성 후 파일 닫기\n",
    "\n",
    "glob.glob(file_name) # 해당 위치에 파일이 제대로 생성되었는지 확인 (glob모듈, 해당 파일이 해당위치에 저장되었는지 확인)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 웹 페이지에서 이미지 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 하나의 이미지 내려받기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 439페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests  \n",
    "\n",
    "#url = 'https://www.python.org/static/img/python-logo.png'\n",
    "url = 'https://www.python.org/static/img/python-logo@2x.png'\n",
    "html_image = requests.get(url)   # 소스코드 복사와 동일한 방법으로 이미지 저장\n",
    "html_image    # 파일이 받아짐 (response 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 440페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python-logo@2x.png'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#파일이름을 변수에 저장\n",
    "import os   # os 패키지 호출\n",
    "\n",
    "image_file_name = os.path.basename(url)   # os 패키지의 path모듈의 basename() 메소드로 url주소에서 파일이름만 남김\n",
    "image_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 441페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#폴더위치를 변수에 저장\n",
    "folder = 'C:/myPyCode/download' \n",
    "\n",
    "if not os.path.exists(folder):   # 해당폴더가 존재하지 않으면,\n",
    "    os.makedirs(folder)          # 입력된 주소에 해당폴더를 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 441페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/myPyCode/download\\\\python-logo@2x.png'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = os.path.join(folder, image_file_name)  # path모듈의 join()메소드로 폴더위치+파일이름을 변수에 저장\n",
    "image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 441페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageFile = open(image_path, 'wb')  # 이미지파일을 저장할때는 wb로 써주어야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 442페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 데이터를 100000 바이트씩 나눠서 내려받고 파일에 순차적으로 저장\n",
    "\n",
    "chunk_size = 1000000  # 사이즈단위를 1mb로 설정\n",
    "\n",
    "for chunk in html_image.iter_content(chunk_size):  # html_image.iter_content()함수로 이미지를 청크단위로 받아옴(반복)\n",
    "    imageFile.write(chunk)\n",
    "imageFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 442페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abendstimmung-4353170__340.jpg',\n",
       " 'banff-4341560__340.jpg',\n",
       " 'cat-2291039__340.jpg',\n",
       " 'danxia-3605276_1280.jpg',\n",
       " 'danxia-3605276__340.jpg',\n",
       " 'indoors-3117027__340.jpg',\n",
       " 'logo.png',\n",
       " 'peony-4243278__340.jpg',\n",
       " 'pixabay',\n",
       " 'python-logo.png',\n",
       " 'python-logo@2x.png',\n",
       " 'samburu-4371555__340.jpg']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(folder)  # 폴더 안에 있는 파일을 보여줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 442 ~ 443페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests    # 이미지 불러올 때 사용\n",
    "import os          # 이미지 저장할 때 사용\n",
    "\n",
    "url = 'https://www.python.org/static/img/python-logo.png'   # 이미지의 url 주소\n",
    "html_image = requests.get(url)                 # 이미지 복사하기\n",
    "image_file_name = os.path.basename(url)        # 이미지 이름\n",
    "\n",
    "folder = 'C:/myPyCode/download'                # 저장위치\n",
    "\n",
    "if not os.path.exists(folder):                # 저장위치에 폴더가 없으면 만들기\n",
    "    os.makedirs(folder)\n",
    "\n",
    "image_path = os.path.join(folder, image_file_name)  # 저장경로(저장위치+이미지이름)\n",
    "\n",
    "imageFile = open(image_path, 'wb')                  # 파일 열기\n",
    "# 이미지 데이터를 100000 바이트씩 나눠서 저장\n",
    "chunk_size = 1000000                                # 저장할 파일 단위\n",
    "for chunk in html_image.iter_content(chunk_size):   # 단위대로 파일 쓰기\n",
    "    imageFile.write(chunk)\n",
    "imageFile.close()                                   # 파일 닫기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 여러 이미지 내려받기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 446페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<img alt=\"\" src=\"https://cdn.pixabay.com/photo/2018/03/29/11/59/snow-3272072__340.jpg\" srcset=\"https://cdn.pixabay.com/photo/2018/03/29/11/59/snow-3272072__340.jpg 1x, https://cdn.pixabay.com/photo/2018/03/29/11/59/snow-3272072__480.jpg 2x\"/>,\n",
       " <img alt=\"\" src=\"https://cdn.pixabay.com/photo/2019/09/16/17/18/spa-4481538__340.jpg\" srcset=\"https://cdn.pixabay.com/photo/2019/09/16/17/18/spa-4481538__340.jpg 1x, https://cdn.pixabay.com/photo/2019/09/16/17/18/spa-4481538__480.jpg 2x\"/>,\n",
       " <img alt=\"\" src=\"https://cdn.pixabay.com/photo/2019/10/10/22/15/norway-4540666__340.jpg\" srcset=\"https://cdn.pixabay.com/photo/2019/10/10/22/15/norway-4540666__340.jpg 1x, https://cdn.pixabay.com/photo/2019/10/10/22/15/norway-4540666__480.jpg 2x\"/>]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests     # 이미지 불러올 때 사용\n",
    "from bs4 import BeautifulSoup    # 소스코드 추출할 때 사용\n",
    "\n",
    "URL = 'https://pixabay.com/ko/photos/?order=popular&cat=animals'  # 파일 주소\n",
    "\n",
    "html_pixabay_image = requests.get(URL).text     # 소스코드 복사하기\n",
    "soup_pixabay_image = BeautifulSoup(html_pixabay_image, \"lxml\")  # 소스코드 파싱하여 소스코드 객체 생성\n",
    "pixabay_image_elements = soup_pixabay_image.select('img')     # 소스코드 객체에서 이미지태그만 변수에 리스트로 저장\n",
    "pixabay_image_elements[0:3]                                   # 3개만 출력해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 446페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://cdn.pixabay.com/photo/2018/03/29/11/59/snow-3272072__340.jpg'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixabay_image_url = pixabay_image_elements[0].get('src')   # get('')로 src 부분의 이미지 url만 추출\n",
    "pixabay_image_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 447페이지]**\n",
    "### 여기서부터 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_image = requests.get(pixabay_image_url)  # 이미지 주소에서 이미지 불러오기\n",
    "\n",
    "folder = \"C:/myPyCode/download/pixabay\"\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder) \n",
    "\n",
    "imageFile = open(os.path.join(folder, os.path.basename(pixabay_image_url)), 'wb')  # 해당 경로에 파일 열기 (시작)\n",
    "#os.path.join(folder, os.path.basename(pixabay_image_url) 폴더위치+url에서 분리한 파일이름을 파일위치로 설정\n",
    "\n",
    "# 이미지 데이터를 100000 바이트씩 나눠서 저장\n",
    "chunk_size = 1000000 \n",
    "for chunk in html_image.iter_content(chunk_size):\n",
    "    imageFile.write(chunk)                          # (실행)\n",
    "imageFile.close()                          # (끝)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 447 ~ 449페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 파일명: 'snow-3272072__340.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'spa-4481538__340.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'norway-4540666__340.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'exotic-4651348__340.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'christmas-4646421__340.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'table-791149__340.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'san-francisco-4674351__340.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'leaves-4673997__340.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'band-4671748__340.jpg'. 내려받기 완료!\n",
      "이미지 파일명: 'outdoor-4635267__340.jpg'. 내려받기 완료!\n",
      "================================\n",
      "선택한 모든 이미지 내려받기 완료!\n"
     ]
    }
   ],
   "source": [
    "import requests  # 소스코드 복사, 파일 불러올때 사용\n",
    "from bs4 import BeautifulSoup   # 소스코드 파싱할때 사용\n",
    "import os        # 경로 설정할 때 사용\n",
    "\n",
    "# URL(주소)에서 이미지 주소 추출하는 함수 생성\n",
    "def get_image_url(url): \n",
    "    html_image_url = requests.get(url).text     # url주소의 소스코드를 텍스트로 저장\n",
    "    soup_image_url = BeautifulSoup(html_image_url, \"lxml\")   # 소스코드 텍스트를 파싱하여 뷰티풀소프객체로 저장\n",
    "    image_elements = soup_image_url.select('img')     # 소스코드 객체에서 img태그만 불러서 리스트에 저장\n",
    "    \n",
    "    if(image_elements != None):    # 리스트가 비어있지 않다면(든게 있다면),\n",
    "        image_urls = []            # 빈 리스트를 만들고,\n",
    "        for image_element in image_elements:   # 각 이미지태그를 반복\n",
    "            image_urls.append(image_element.get('src'))  # src의 주소만 가져와서 빈 리스트에 담는다\n",
    "        return image_urls        # 이미지 주소만 담기 리스트를 리턴함\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "# 폴더를 지정해 이미지 주소에서 이미지 내려받는 함수 생성\n",
    "def download_image(img_folder, img_url):   # 저장위치, url주소를 파라미터로 받음\n",
    "    if(img_url != None):   # 파라미터에 입력값이 없지 않다면(주소를 입력했다면),\n",
    "        html_image = requests.get(img_url)  # url주소를 소스코드로 복사\n",
    "        \n",
    "        # os.path.basename(URL)는 웹사이트나 폴더가 포함된 파일명에서 파일명만 분리    \n",
    "        imageFile = open(os.path.join(img_folder, os.path.basename(img_url)), 'wb') # 저장위치+url주소에서분리한 파일이름을 합쳐\n",
    "                                                                                    # 파일을 열기 (시작)\n",
    "\n",
    "        chunk_size = 1000000 # 이미지 데이터를 100000 바이트씩 나눠서 저장\n",
    "        for chunk in html_image.iter_content(chunk_size):   # 반복문 실행\n",
    "            imageFile.write(chunk)                  # 파일 다운로드 (실행)\n",
    "            imageFile.close()                   # 파일 다운로드 끝\n",
    "        print(\"이미지 파일명: '{0}'. 내려받기 완료!\".format(os.path.basename(img_url)))  # 파일 다운로드가 완료될 때마다 프린트 실행  \n",
    "    else:       \n",
    "        print(\"내려받을 이미지가 없습니다.\")  # 다운받은게 없을 때 프린트 실행\n",
    "        \n",
    "  \n",
    "pixabay_url = 'https://pixabay.com/ko/photos/?order=popular&cat=animals'   # 웹 사이트의 주소 지정 \n",
    "# pixabay_url= 'https://pixabay.com/ko/photos/?order=popular&cat=animals&pagi=2'\n",
    "\n",
    "figure_folder = \"C:/myPyCode/download/pixabay\" # 이미지를 내려받을 폴더 지정  \n",
    "\n",
    "\n",
    "\n",
    "pixabay_image_urls = get_image_url(pixabay_url) # 함수를 실행해서 해당 주소의 이미지 주소만 담기 리스트 출력\n",
    "\n",
    "\n",
    "\n",
    "num_of_download_image = 10# 내려받을 이미지 개수 지정\n",
    "# num_of_download_image = len(pixabay_image_urls)\n",
    "\n",
    "for k in range(num_of_download_image):\n",
    "    download_image(figure_folder,pixabay_image_urls[k])\n",
    "print(\"================================\")\n",
    "print(\"선택한 모든 이미지 내려받기 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[14장: 450페이지]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_download_image = len(pixabay_image_urls)\n",
    "num_of_download_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.4 정리"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nav_menu": {},
  "toc": {
   "nav_menu": {
    "height": "400px",
    "width": "252px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "545px",
    "left": "0px",
    "right": "1154px",
    "top": "111px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "toc_position": {
   "height": "503px",
   "left": "0px",
   "right": "952.167px",
   "top": "107px",
   "width": "300px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
